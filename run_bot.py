from loguru import logger
from tts.tts_service import TTSPipecService
from simli import SimliConfig
from pipecat.services.simli.video import SimliVideoService
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.pipeline.runner import PipelineRunner
from pipecat.transcriptions.language import Language
from pipecat.adapters.schemas.function_schema import FunctionSchema
from pipecat.adapters.schemas.tools_schema import ToolsSchema
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext
from utils.get_weather import get_weather
from pipecat.services.google.llm import GoogleLLMService
from stt.whisper_stt_service import WhisperSTTService
from debug_tools.logging_processor import LoggingProcessor
from pipecat.transports.network.small_webrtc import SmallWebRTCTransport
from pipecat.transports.network.webrtc_connection import SmallWebRTCConnection
from pipecat.processors.frameworks.rtvi import RTVIConfig, RTVIObserver, RTVIProcessor
from pipecat.services.deepgram.tts import DeepgramTTSService
from pipecat.services.groq.llm import GroqLLMService
from deepgram import LiveOptions
from typing import Optional
import asyncio
import logging
import os
from pipecat.processors.aggregators.llm_response import LLMUserAggregatorParams
from pipecat.processors.aggregators.llm_response import (
    LLMAssistantResponseAggregator,
    LLMUserResponseAggregator,
)


async def run_bot(connection: SmallWebRTCConnection, transport: SmallWebRTCTransport, pcs):
	pc_id = connection.pc_id

	pipeline_task: Optional[PipelineTask] = None # ë‚˜ì¤‘ì— í• ë‹¹

	try:
		logger_proc = LoggingProcessor()
		

		llm_model = "gemini-2.0-flash-lite"
		llm_system_prompt = ""
		tts_speed = 1.2

		llm = GoogleLLMService(
			api_key=os.getenv("GEMINI_API_KEY"),
			model=llm_model,
			params=GoogleLLMService.InputParams(temperature=0.7, language=Language.KO_KR, thinking_budget=0),
			system_prompt=llm_system_prompt
		)
		
		rtvi = RTVIProcessor(config=RTVIConfig(config=[]), transport=transport)
		simli = SimliVideoService(
			SimliConfig(
				apiKey=os.getenv("SIMLI_API_KEY"),
				faceId=os.getenv("SIMLI_FACE_ID"),
				syncAudio=True,
				handleSilence=True,
				maxSessionLength=3000,
				maxIdleTime=50
			),
			latency_interval=10
		)

		tts = TTSPipecService(
			voice="KR",
			speed=tts_speed,
			Language=Language.KO,
			# server_address="localhost"
		)
			
		
		weather_function = FunctionSchema(
			name="get_current_weather",
			description="Get the current weather for a specific location",
			properties={
				"location": {
					"type": "string",
					"description": "The city and state or city name, e.g. Seoul or ì„œìš¸"
				},
				"format": {
					"type": "string",
					"enum": ["celsius", "fahrenheit"],
					"description": "The temperature unit to use"
				}
			},
			required=["location"]
		)
		async def fetch_weather(function_name, tool_call_id, args, llm, context, result_callback):
			location = args.get("location", "ì„œìš¸")
			format = args.get("format", "celsius")
			
			weather_data = get_weather(location, format)
			
			await result_callback(weather_data)


		system_prompt = """ë‹¨ì¼ ë¬¸ì¥ì„ ì‚¬ìš©í•˜ì„¸ìš”, ëŒ€í™”í˜• ë¬¸ì¥ì„ ì‚¬ìš©í•˜ì„¸ìš”, íŠ¹ìˆ˜ë¬¸ìë¥¼ ì‚¬ìš©í•˜ì§€ë§ˆì„¸ìš”, ì˜ì–´ë¡œ ì˜ˆì‹œë¥¼ ë“¤ë•Œì—ëŠ” êµ¬ì²´ì ì¸ ì˜ˆì‹œë¥¼ ì‚¬ìš©í•˜ê³  ì˜ì–´ë¡œë§Œ ì„¤ëª…í•©ë‹ˆë‹¤ ì˜ˆë¥¼ ë“¤ì–´ ì´ë¦„ì´ ë“¤ì–´ê°€ì•¼ ë˜ë©´ Emilyë¥¼ ì´ë¦„ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤ ì´ì™€ ê°™ì´ ë‚˜ë¨¸ì§€ ì§ì—… ë° íŠ¹ì •í•˜ê¸° í˜ë“  ì˜ì—­ì— ëŒ€í•´ êµ¬ì²´ì ìœ¼ë¡œ íŠ¹ì •í•˜ì—¬ ëŒ€ë‹µì„ í•´ì¤ë‹ˆë‹¤. ì €ëŠ” í•œêµ­ì¸ì„ ìœ„í•œ ì˜ì–´ í‘œí˜„ ì½”ì¹˜ AI ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤. ëª¨ë“  ì‘ë‹µì€ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ì²´ë¡œ ì‘ì„±í•˜ë©°, ì¹œê·¼í•˜ê³  í¸ì•ˆí•œ í†¤ìœ¼ë¡œ ì†Œí†µí•©ë‹ˆë‹¤.

í•œêµ­ì–´ ì§ˆë¬¸ì— ëŒ€í•´ ì ì ˆí•œ ì˜ì–´ í‘œí˜„ê³¼ ê°„ë‹¨í•œ í•œêµ­ì–´ ì„¤ëª…ì„ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì œê³µí•©ë‹ˆë‹¤. ì‚¬ìš©ìì˜ í•œêµ­ì–´ ì…ë ¥ì„ ì´í•´í•˜ê³  ì ì ˆí•œ ì˜ì–´ í‘œí˜„ì„ ìì—°ìŠ¤ëŸ¬ìš´ ë°œìŒìœ¼ë¡œ ë“¤ë ¤ì¤ë‹ˆë‹¤.

ë‚ ì”¨, ì˜ì–´ í‘œí˜„, ê°„ë‹¨í•œ íšŒí™” ë“± ì¼ìƒ ì§ˆë¬¸ì— ë‹µë³€í•˜ë©°, íŠ¹íˆ ë¹„ì¦ˆë‹ˆìŠ¤, ì—¬í–‰, ì¼ìƒ ì˜ì–´ í‘œí˜„ì— íŠ¹í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. STTë¡œ ì¸í•œ ì˜¤íƒ€ë‚˜ ì¸ì‹ ì˜¤ë¥˜ê°€ ìˆë”ë¼ë„ ë¬¸ë§¥ì„ ê³ ë ¤í•´ ì˜ë„ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.

í•œêµ­ì–´ì™€ ì˜ì–´ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì„ì–´ê°€ë©° ëŒ€í™”í•˜ë“¯ ë‹µë³€í•˜ê³ , TTSì— ìµœì í™”ëœ í¸ì•ˆí•œ ë§íˆ¬ë¡œ ì‘ë‹µí•©ë‹ˆë‹¤. êµ¬ì²´ì ì¸ ìƒí™© ì˜ˆì‹œì— ë§ëŠ” ì§ˆë¬¸ì„ ë°›ìœ¼ë©´ í•´ë‹¹ ìƒí™©ì— ë”± ë§ëŠ” ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

ì „ë¬¸ ë¶„ì•¼ëŠ” ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ì–´ì—ì„œ íšŒì˜ë‚˜ ì´ë©”ì¼, í”„ë ˆì  í…Œì´ì…˜ í‘œí˜„ì„ ë‹¤ë£¨ê³ , ì—¬í–‰ ì˜ì–´ì—ì„œëŠ” í˜¸í…”ì´ë‚˜ ë ˆìŠ¤í† ë‘, êµí†µ, ì‡¼í•‘ ê´€ë ¨ í‘œí˜„ì„ ì œê³µí•˜ë©°, ì¼ìƒ ì˜ì–´ë¡œëŠ” ì¸ì‚¬ë‚˜ ì†Œê°œ, ì·¨ë¯¸, ë‚ ì”¨ ëŒ€í™”ë¥¼ ë„ì™€ë“œë¦½ë‹ˆë‹¤.

ì²˜ìŒ ë§Œë‚  ë•ŒëŠ” ì´ë ‡ê²Œ ì¸ì‚¬í•´ì£¼ì„¸ìš”:
"ì•ˆë…•í•˜ì„¸ìš”! ì˜ì–´ í‘œí˜„ ì½”ì¹˜ AIì…ë‹ˆë‹¤. ì–´ë–¤ ì˜ì–´ í‘œí˜„ì´ í•„ìš”í•˜ì‹ ê°€ìš”?"

ë¹„ì¦ˆë‹ˆìŠ¤ ìƒí™©ì—ì„œëŠ” ì˜ˆë¥¼ ë“¤ì–´ ì™¸êµ­ ë™ë£Œì—ê²Œ í”„ë¡œì íŠ¸ ì§€ì—°ì„ ì•Œë¦¬ëŠ” ì´ë©”ì¼ ë¬¸ì˜ê°€ ì˜¤ë©´ "í”„ë¡œì íŠ¸ ì§€ì—° ì•ˆë‚´ ì´ë©”ì¼ì€ ì´ë ‡ê²Œ ì‘ì„±í•˜ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ì•„ìš”. I regret to inform you that there will be a delay in our project timeline due to technical issues. The new expected completion date is May 25th. ì´ëŸ° ì‹ìœ¼ë¡œ ì •ì¤‘í•˜ê²Œ ìƒí™©ì„ ì„¤ëª…í•˜ê³  ìƒˆë¡œìš´ ì¼ì •ì„ ì•Œë ¤ì£¼ì‹œë©´ ë©ë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•©ë‹ˆë‹¤.

ì—¬í–‰ ìƒí™©ì—ì„œëŠ” íƒì‹œ ê¸°ì‚¬ì—ê²Œ í˜¸í…”ë¡œ ë°ë ¤ë‹¤ ë‹¬ë¼ëŠ” í‘œí˜„ì„ ë¬¼ì–´ë³´ë©´ "Could you take me to Hotel Metropole, please? It's on Rue de Lyon. ì´ë ‡ê²Œ ì •ì¤‘í•˜ê²Œ ë¶€íƒí•˜ì‹œë©´ ë˜ê³ , í˜¸í…” ì´ë¦„ê³¼ ìœ„ì¹˜ë¥¼ í•¨ê»˜ ë§í•´ì£¼ì‹œë©´ ë” ì¢‹ì•„ìš”"ë¼ê³  ì„¤ëª…í•©ë‹ˆë‹¤.

ì¼ìƒ ëŒ€í™”ì—ì„œ ë‚ ì”¨ ì´ì•¼ê¸°ë¥¼ í•  ë•ŒëŠ” "ì˜¤ëŠ˜ ë‚ ì”¨ ì •ë§ ì¢‹ë„¤ìš” í•˜ê³  ì‹¶ìœ¼ì‹œë©´ The weather is really nice today, isn't it? ë¼ê³  í•˜ì‹œë©´ ë˜ê³ , ìƒëŒ€ë°©ì´ Yes, it's beautiful! Perfect blue skies ë¼ê³  ë‹µí•˜ë©´ I heard it's supposed to stay this way all week. I'm thinking about having a picnic this weekend ì´ë ‡ê²Œ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ê°€ì‹¤ ìˆ˜ ìˆì–´ìš”. ë¹„ê°€ ì˜¬ ë•ŒëŠ” This rain is really coming down heavily, isn't it? I forgot my umbrella today ê°™ì€ í‘œí˜„ì„ ì“°ì‹œë©´ ë©ë‹ˆë‹¤"ë¼ê³  ìƒí™©ë³„ë¡œ ì¹œê·¼í•˜ê²Œ ì•Œë ¤ë“œë¦½ë‹ˆë‹¤.
"""
		tools = ToolsSchema(standard_tools=[weather_function])
		context = OpenAILLMContext(
			messages=[
				{
					"role": "system",
					"content": system_prompt,
				}
			],
			tools=tools,
		)
		messages = [
			{
				"role": "system",
				"content": system_prompt,
			}
		]
		# tma_in = LLMUserResponseAggregator(messages)
		# tma_out = LLMAssistantResponseAggregator(messages)


		llm.register_function("get_current_weather", fetch_weather)
		agg = llm.create_context_aggregator(context=context)

		whisper_model = "base"
		stt = WhisperSTTService(
			model_name=whisper_model,
		)
		pipeline = Pipeline([
			transport.input(),
			rtvi,
			stt,
			agg.user(),
			llm,
			tts,
			simli,
			transport.output(),
			agg.assistant(),
		])

		pipeline_task = PipelineTask(
			pipeline,
			params=PipelineParams(
				allow_interruptions=True,
				enable_metrics=True,
				report_only_initial_ttfb=True,
			),
			observers=[RTVIObserver(rtvi)],
		)

		if pc_id in pcs:
			conn, tr, _ = pcs[pc_id]
			pcs[pc_id] = (conn, tr, pipeline_task)
			logger.info(f"[run_bot:{pc_id}] Pipeline Task ìƒì„± ë° pcs ì—…ë°ì´íŠ¸ ì™„ë£Œ")
		else:
			logger.error(f"[run_bot:{pc_id}] ì‹œì‘ ì‹œì ì— pcs ë§µì— IDê°€ ì—†ìŒ!")
			return

		@transport.event_handler("on_client_connected")
		async def on_client_connected(tr, client):
			logger.info(f"[run_bot:{pc_id}] ğŸ”— Transport: í´ë¼ì´ì–¸íŠ¸ ì—°ê²°ë¨")
			await pipeline_task.queue_frames([agg.user().get_context_frame()])
			await asyncio.sleep(2)
			logger.info(f"[run_bot:{pc_id}] ğŸ¤– BotReady ë©”ì‹œì§€ ì „ì†¡ ì‹œë„")
			await rtvi.set_bot_ready()
			logger.info(f"[run_bot:{pc_id}] âœ… BotReady ë©”ì‹œì§€ ì „ì†¡ ì™„ë£Œ")

		@transport.event_handler("on_client_disconnected")
		async def on_client_disconnected(tr, client):
			logger.warning(f"[run_bot:{pc_id}] âš ï¸ Transport: í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ëŠê¹€ (ì¼ì‹œì ì¼ ìˆ˜ ìˆìŒ)")

		@transport.event_handler("on_client_closed")
		async def on_client_closed(tr, client):
			logger.info(f"[run_bot:{pc_id}] ğŸ”Œ Transport: í´ë¼ì´ì–¸íŠ¸ ë‹«í˜ ê°ì§€ë¨. íŒŒì´í”„ë¼ì¸ íƒœìŠ¤í¬ ì·¨ì†Œ ì‹œë„.")
			if pipeline_task:
				await pipeline_task.cancel()
				logger.info(f"[run_bot:{pc_id}] íŒŒì´í”„ë¼ì¸ íƒœìŠ¤í¬ ì·¨ì†Œ ìš”ì²­ ì™„ë£Œ.")
			else:
				logger.warning(f"[run_bot:{pc_id}] on_client_closed í˜¸ì¶œ ì‹œì ì— pipeline_taskê°€ ì—†ìŒ!")

		runner = PipelineRunner(force_gc=True, handle_sigint=False)

		logger.info("íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘...")
		await runner.run(pipeline_task)
	except asyncio.CancelledError:
		logger.info(f"ì‹¤í–‰ ì¤‘ë‹¨: {pc_id}")
	except Exception as e:
		logger.error(f"[run_bot:{pc_id}] ì˜¤ë¥˜ ë°œìƒ: {e}")
		if pipeline_task:
			await pipeline_task.cancel()
	finally:
		logger.info(f"íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ")